{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk3viDnv936z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize and reshape the input images\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "def create_autoencoder(input_shape):\n",
        "    input_img = Input(shape=input_shape)\n",
        "    encoded = Flatten()(input_img)\n",
        "    encoded = Dense(128, activation='relu')(encoded)\n",
        "    decoded = Dense(np.prod(input_shape), activation='sigmoid')(encoded)\n",
        "    decoded = Reshape(input_shape)(decoded)\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    return autoencoder\n",
        "\n",
        "\n",
        "\n",
        "# Create and train the first autoencoder\n",
        "autoencoder1 = create_autoencoder((28, 28, 1))\n",
        "autoencoder1.fit(x_train, x_train, epochs=10, batch_size=128)\n",
        "\n",
        "# Create and train the second autoencoder\n",
        "autoencoder2 = create_autoencoder((28, 28, 1))\n",
        "autoencoder2.fit(autoencoder1.predict(x_train), autoencoder1.predict(x_train), epochs=10, batch_size=128)\n",
        "\n",
        "def generate_fgsm_adversarial(model, x, y, epsilon=0.01):\n",
        "    x_adv = tf.constant(x, dtype=tf.float32)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x_adv)\n",
        "        logits = model(x_adv, training=False)\n",
        "        loss = keras.losses.sparse_categorical_crossentropy(y, logits)\n",
        "\n",
        "    gradient = tape.gradient(loss, x_adv)\n",
        "    signed_grad = tf.sign(gradient)\n",
        "    x_adv = tf.clip_by_value(x_adv + epsilon * signed_grad, 0, 1)\n",
        "\n",
        "    return x_adv.numpy()\n",
        "\n",
        "def generate_pgd_adversarial(model, x, y, epsilon=0.01, alpha=0.01, num_iter=40):\n",
        "    x_adv = tf.Variable(x, dtype=tf.float32)\n",
        "\n",
        "    for _ in range(num_iter):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x_adv)\n",
        "            logits = model(x_adv, training=False)\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(y, logits)\n",
        "\n",
        "        gradient = tape.gradient(loss, x_adv)\n",
        "        signed_grad = tf.sign(gradient)\n",
        "        x_adv.assign_add(alpha * signed_grad)\n",
        "        x_adv.assign(tf.clip_by_value(x_adv, x - epsilon, x + epsilon))\n",
        "        x_adv.assign(tf.clip_by_value(x_adv, 0, 1))\n",
        "\n",
        "    return x_adv.numpy()\n",
        "\n",
        "  from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "classifier = keras.Sequential([\n",
        "    Flatten(input_shape=(28, 28, 1)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "# Compile and train the classifier on the MNIST dataset\n",
        "classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "classifier.fit(x_train, y_train, epochs=5, batch_size=128)\n",
        "\n",
        "\n",
        "# Generate FGSM adversarial images and denoise them\n",
        "epsilon = 0.1  # Adjust the epsilon value as desired\n",
        "adv_images_fgsm = generate_fgsm_adversarial(classifier, x_test, y_test, epsilon)\n",
        "denoised_images1_fgsm = autoencoder1.predict(adv_images_fgsm)\n",
        "denoised_images2_fgsm = autoencoder2.predict(denoised_images1_fgsm)\n",
        "\n",
        "# Generate PGD adversarial images and denoise them\n",
        "epsilon = 0.1  # Adjust the epsilon value as desired\n",
        "alpha = 0.01  # Adjust the alpha value as desired\n",
        "num_iter = 40  # Adjust the number of iterations as desired\n",
        "adv_images_pgd = generate_pgd_adversarial(classifier, x_test, y_test, epsilon, alpha, num_iter)\n",
        "denoised_images1_pgd = autoencoder1.predict(adv_images_pgd)\n",
        "denoised_images2_pgd = autoencoder2.predict(denoised_images1_pgd)\n",
        "\n",
        "# Generate CW blackbox adversarial images and denoise them\n",
        "epsilon = 0.1  # Adjust the epsilon value as desired\n",
        "num_iter = 40  # Adjust the number of iterations as desired\n",
        "confidence = 0.0  # Adjust the confidence value as desired\n",
        "\n",
        "# adv_images_cw = generate_cw_blackbox_adversarial(classifier, x_test, y_test, epsilon, num_iter, confidence)\n",
        "# denoised_images1_cw = autoencoder1.predict(adv_images_cw)\n",
        "# denoised_images2_cw = autoencoder2.predict(denoised_images1_cw)\n",
        "\n",
        "# Evaluate the classifier on the original adversarial images\n",
        "original_adv_acc_fgsm = classifier.evaluate(adv_images_fgsm, y_test, verbose=0)[1]\n",
        "original_adv_acc_pgd = classifier.evaluate(adv_images_pgd, y_test, verbose=0)[1]\n",
        "#original_adv_acc_cw = classifier.evaluate(adv_images_cw, y_test, verbose=0)[1]\n",
        "print('Accuracy on original adversarial image (FGSM):', original_adv_acc_fgsm)\n",
        "print('Accuracy on original adversarial image (PGD):', original_adv_acc_pgd)\n",
        "#print('Accuracy on original adversarial image (CW):', original_adv_acc_cw)\n",
        "\n",
        "# Evaluate the classifier on the denoised images\n",
        "print('Accuracy on denoised image after using two autoencoders: ')\n",
        "denoised_acc_fgsm1 = classifier.evaluate(denoised_images1_fgsm, y_test, verbose=0)[1]\n",
        "denoised_acc_pgd1 = classifier.evaluate(denoised_images1_pgd, y_test, verbose=0)[1]\n",
        "#denoised_acc_cw1 = classifier.evaluate(denoised_images1_cw, y_test, verbose=0)[1]\n",
        "print('Accuracy on denoised image (FGSM):', denoised_acc_fgsm1)\n",
        "print('Accuracy on denoised image (PGD):', denoised_acc_pgd1)\n",
        "#print('Accuracy on denoised image (CW):', denoised_acc_cw1)\n",
        "\n",
        "\n",
        "# Evaluate the classifier on the denoised images\n",
        "denoised_acc_fgsm = classifier.evaluate(denoised_images2_fgsm, y_test, verbose=0)[1]\n",
        "denoised_acc_pgd = classifier.evaluate(denoised_images2_pgd, y_test, verbose=0)[1]\n",
        "#denoised_acc_cw = classifier.evaluate(denoised_images2_cw, y_test, verbose=0)[1]\n",
        "print('Accuracy on denoised image after using one autoencoders: ')\n",
        "print('Accuracy on denoised image (FGSM):', denoised_acc_fgsm)\n",
        "print('Accuracy on denoised image (PGD):', denoised_acc_pgd)\n",
        "#print('Accuracy on denoised image (CW):', denoised_acc_cw)"
      ]
    }
  ]
}